{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE542-Competition Final",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2AGfKfiHBHH"
      },
      "source": [
        "**Google Drive Connection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15caHsxsMzNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1329290-9561-40ae-b71d-0ed873b0df07"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import properties\n",
        "import data_utils\n",
        "import model_utils\n",
        "import logger\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout, Bidirectional, LSTM, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "print(properties.DATA_HOME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CSC/ECE542/Competition/data/TrainingData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB__NQiOHbCB"
      },
      "source": [
        "**Data import and preprocess includes one hot encoding of y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3QHYcf3OJ3G",
        "outputId": "96adcacf-fcd2-4d20-c273-fe730a3b1e9d"
      },
      "source": [
        "# Assigns data path\n",
        "DATA_HOME = properties.DATA_HOME\n",
        "TEST_HOME = properties.TEST_HOME\n",
        "LOGGER = logger.get_logger(\"ECE 542\")\n",
        "\n",
        "# Segregates data into training, validation and test records\n",
        "training_records = [\n",
        "                    \"subject_001_01__\", \"subject_001_02__\", \"subject_001_03__\", \"subject_001_04__\", \"subject_001_05__\", \"subject_001_06__\", \"subject_001_07__\",\n",
        "                    \"subject_002_01__\", \"subject_002_02__\", \"subject_002_03__\", \"subject_002_04__\",\n",
        "                    \"subject_003_01__\", \"subject_003_02__\",\n",
        "                    \"subject_004_01__\",\n",
        "                    \"subject_005_01__\", \"subject_005_02__\",\n",
        "                    \"subject_006_01__\", \"subject_006_02__\",\n",
        "                    \"subject_007_01__\", \"subject_007_02__\", \"subject_007_03__\",\n",
        "                    \"subject_008_01__\"\n",
        "                    ]\n",
        "\n",
        "validation_records = [\"subject_001_08__\",\n",
        "                      \"subject_002_05__\",\n",
        "                      \"subject_003_03__\",\n",
        "                      \"subject_004_02__\",\n",
        "                      \"subject_005_03__\",\n",
        "                      \"subject_006_03__\",\n",
        "                      \"subject_007_04__\"]\n",
        "\n",
        "\n",
        "test_records = [\"subject_009_01__\",\n",
        "                \"subject_010_01__\",\n",
        "                \"subject_011_01__\",\n",
        "                \"subject_012_01__\"]\n",
        "\n",
        "# Sampling Rate dictionary to convert rate to sampling_rate object\n",
        "sampling_rates = {\n",
        "  \"1\": data_utils.SamplingRate([-0.02], 0, 0, 4),\n",
        "  \"2\": data_utils.SamplingRate([-0.045, -0.02], 0, 1, 4),\n",
        "  \"4\": data_utils.SamplingRate([-0.07, -0.045, -0.02, 0.005], -2, 1, 4),\n",
        "  \"6\": data_utils.SamplingRate([-0.12, -0.095, -0.07, -0.045, -0.02, 0.005], -4, 1, 4),\n",
        "  \"10\": data_utils.SamplingRate([-0.22, -0.195, -0.17, -0.145, -0.12, -0.095, -0.07, -0.045, -0.02, 0.005], -8, 1, 4),\n",
        "  \"30\": data_utils.SamplingRate([-0.72, -0.695, -0.67, -0.645, -0.62, -0.595, -0.57, -0.545, -0.52, -0.495, -0.47, -0.445, -0.42, -0.395, -0.37, -0.345, -0.32, -0.295, -0.27, -0.245, -0.22, -0.195, -0.17, -0.145, -0.12, -0.095, -0.07, -0.045, -0.02, 0.005], -28, 1, 4),\n",
        "  \"60\": data_utils.SamplingRate([-1.47, -1.445, -1.42, -1.395, -1.37, -1.345, -1.32, -1.295, -1.27, -1.245, -1.22, -1.195, -1.17, -1.145, -1.12, -1.095, -1.07, -1.045, -1.02, -0.995, -0.97, -0.945, -0.92, -0.895, -0.87, -0.845, -0.82, -0.795, -0.77, -0.745, -0.72, -0.695, -0.67, -0.645, -0.62, -0.595, -0.57, -0.545, -0.52, -0.495, -0.47, -0.445, -0.42, -0.395, -0.37, -0.345, -0.32, -0.295, -0.27, -0.245, -0.22, -0.195, -0.17, -0.145, -0.12, -0.095, -0.07, -0.045, -0.02, 0.005], -58, 1, 4)\n",
        "}\n",
        "\n",
        "# Using sampling rate (selected to 30 after several runs).\n",
        "sampling_rate = sampling_rates['30']\n",
        "\n",
        "# Importing data and preprocessing data according to sampling rate 30 and getting weights for training and validation\n",
        "training_data_files = data_utils.get_data_files(DATA_HOME, training_records)\n",
        "training_stream = data_utils.DataStreamer(training_data_files, sample_deltas=sampling_rate, do_shuffle=False,\n",
        "                                          class_balancer=None, batch_size=1)\n",
        "train_x, train_y, train_sample_weights = training_stream.preprocess()\n",
        "validation_data_files = data_utils.get_data_files(DATA_HOME, validation_records)\n",
        "validation_stream = data_utils.DataStreamer(validation_data_files, sample_deltas=sampling_rate, do_shuffle=False,\n",
        "                                            class_balancer=None, batch_size=1)\n",
        "valid_x, valid_y, valid_sample_weights = validation_stream.preprocess(n_classes=len(training_stream.classes))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-04-21 16:11:59,030] [INFO] [data_utils.py] Loading data from files .... \n",
            "[2021-04-21 16:12:50,578] [INFO] [data_utils.py] Sampling data: Counter({'0': 200369, '3': 35542, '2': 15036, '1': 11325})\n",
            "[2021-04-21 16:12:50,855] [INFO] [data_utils.py] Loading data from files .... \n",
            "[2021-04-21 16:13:04,719] [INFO] [data_utils.py] Sampling data: Counter({'0': 51364, '3': 16067, '2': 3231, '1': 2479})\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfJ2wTwyA3O6",
        "outputId": "ac2466b4-8974-4dd0-8b02-1499b8383ec1"
      },
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(262272, 30, 6)\n",
            "(262272, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoyxmpG5JT20"
      },
      "source": [
        "**Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhIJuWDNYbP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df8e940-6813-4078-ad34-03a09fe862f8"
      },
      "source": [
        "\n",
        "# NN Model\n",
        "# Intialize sequential model\n",
        "model = Sequential()\n",
        "# Add Bidirectional LSTM layer\n",
        "model.add(\n",
        "    Bidirectional(\n",
        "      LSTM(\n",
        "          units=128, \n",
        "          input_shape=[train_x.shape[1], train_x.shape[2]] ,return_sequences=True\n",
        "      )\n",
        "    )\n",
        ")\n",
        "# Add droput layer with rate 0.5\n",
        "model.add(Dropout(rate=0.5))\n",
        "# Add timedistributed dense layer with relu activation\n",
        "model.add(TimeDistributed(Dense(units=128, activation='relu')))\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "# Add last dense layer with softmax activation\n",
        "model.add(Dense(train_y.shape[1], activation='softmax'))\n",
        "\n",
        "# Compile the model with catetegorical crossentropy loss and adam optimizer and accuracy metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# Fit the model to the data\n",
        "history=model.fit(train_x, train_y,batch_size=100,epochs=20, verbose=1, validation_data=(valid_x, valid_y, valid_sample_weights), sample_weight=train_sample_weights)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2623/2623 [==============================] - 170s 63ms/step - loss: 0.5070 - acc: 0.7134 - val_loss: 0.3637 - val_acc: 0.7919\n",
            "Epoch 2/20\n",
            "2623/2623 [==============================] - 161s 61ms/step - loss: 0.2238 - acc: 0.8727 - val_loss: 0.2711 - val_acc: 0.8188\n",
            "Epoch 3/20\n",
            "2623/2623 [==============================] - 158s 60ms/step - loss: 0.1813 - acc: 0.8924 - val_loss: 0.3076 - val_acc: 0.8176\n",
            "Epoch 4/20\n",
            "2623/2623 [==============================] - 159s 61ms/step - loss: 0.1563 - acc: 0.9035 - val_loss: 0.3431 - val_acc: 0.8491\n",
            "Epoch 5/20\n",
            "2623/2623 [==============================] - 158s 60ms/step - loss: 0.1413 - acc: 0.9104 - val_loss: 0.3131 - val_acc: 0.8143\n",
            "Epoch 6/20\n",
            "2623/2623 [==============================] - 159s 60ms/step - loss: 0.1266 - acc: 0.9186 - val_loss: 0.3058 - val_acc: 0.8364\n",
            "Epoch 7/20\n",
            "2623/2623 [==============================] - 159s 60ms/step - loss: 0.1181 - acc: 0.9246 - val_loss: 0.3430 - val_acc: 0.8126\n",
            "Epoch 8/20\n",
            "2623/2623 [==============================] - 157s 60ms/step - loss: 0.1078 - acc: 0.9295 - val_loss: 0.3446 - val_acc: 0.8430\n",
            "Epoch 9/20\n",
            "2623/2623 [==============================] - 156s 60ms/step - loss: 0.1018 - acc: 0.9326 - val_loss: 0.3913 - val_acc: 0.8521\n",
            "Epoch 10/20\n",
            "2623/2623 [==============================] - 156s 60ms/step - loss: 0.0958 - acc: 0.9369 - val_loss: 0.4717 - val_acc: 0.8408\n",
            "Epoch 11/20\n",
            "2623/2623 [==============================] - 156s 60ms/step - loss: 0.0883 - acc: 0.9415 - val_loss: 0.4697 - val_acc: 0.8248\n",
            "Epoch 12/20\n",
            "2623/2623 [==============================] - 157s 60ms/step - loss: 0.0843 - acc: 0.9436 - val_loss: 0.4564 - val_acc: 0.8077\n",
            "Epoch 13/20\n",
            "2623/2623 [==============================] - 157s 60ms/step - loss: 0.0804 - acc: 0.9469 - val_loss: 0.4351 - val_acc: 0.8329\n",
            "Epoch 14/20\n",
            "2623/2623 [==============================] - 156s 60ms/step - loss: 0.0776 - acc: 0.9481 - val_loss: 0.4474 - val_acc: 0.8290\n",
            "Epoch 15/20\n",
            " 474/2623 [====>.........................] - ETA: 1:58 - loss: 0.0745 - acc: 0.9489"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8x_jmE7XxoE",
        "outputId": "75203496-a177-4073-fa58-fd789a3d29bc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333082795143127"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwO-hEwiOV1J"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afphPC67RO76"
      },
      "source": [
        "# Traverse through all the testing file inputs and predict using the trained model\n",
        "for test_record in test_records:\n",
        "    LOGGER.info(\"Predicting for '%s' ... \" % test_record)\n",
        "    testing_data_file = data_utils.get_data_files(TEST_HOME, [test_record], skip_y=True)\n",
        "    testing_stream = data_utils.DataStreamer(testing_data_file, sample_deltas=sampling_rates['30'], do_shuffle=False)\n",
        "    test_x = testing_stream.features\n",
        "    y_predicted = model.predict(test_x)\n",
        "    test_file_path = os.path.join(TEST_HOME, \"%sy_prediction.csv\" % test_record)\n",
        "    y_test_int = np.argmax(y_predicted, axis=1)\n",
        "\n",
        "    # Saves data to csv\n",
        "    data_utils.dump_labels_to_csv(y_test_int, test_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nfFyWE3eAVJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnUi9vQ2Rcqy",
        "outputId": "ba69cae4-2b92-4664-ed29-3cd20ec0aa1d"
      },
      "source": [
        "training_data_files = data_utils.get_data_files(DATA_HOME, training_records)\n",
        "training_stream = data_utils.DataStreamer(training_data_files, sample_deltas=sampling_rate, do_shuffle=False,\n",
        "                                          class_balancer=balancer, batch_size=1)\n",
        "train_x, train_y, train_sample_weights = training_stream.preprocess()\n",
        "validation_data_files = data_utils.get_data_files(DATA_HOME, validation_records)\n",
        "validation_stream = data_utils.DataStreamer(validation_data_files, sample_deltas=sampling_rate, do_shuffle=False,\n",
        "                                            class_balancer=None, batch_size=1)\n",
        "valid_x, valid_y, valid_sample_weights = validation_stream.preprocess(n_classes=len(training_stream.classes))\n",
        "lstm = model_utils.SimpleLSTM((train_x, train_y, train_sample_weights), (valid_x, valid_y, valid_sample_weights),\n",
        "                              sampling_rate.window_size, training_stream.n_features,\n",
        "                                  len(training_stream.classes), batch_size=batch_size, epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    }
  ]
}